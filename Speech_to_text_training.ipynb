{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09eba5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\app\\python\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wave\n",
    "import contextlib\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import evaluate  # Thay thế load_metric bằng evaluate\n",
    "from datasets import load_dataset, Audio, Dataset\n",
    "from transformers import (Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments,\n",
    "                          Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, DataCollatorWithPadding, AutoConfig)\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d83bec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25708a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sử dụng thiết bị: cpu\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra thiết bị\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Sử dụng thiết bị: {device}\")\n",
    "\n",
    "# Đường dẫn đến dataset VIVOS\n",
    "train_audio_path = 'vivos/train/waves'\n",
    "train_prompts_path = 'vivos/train/prompts.txt'\n",
    "train_genders_path = 'vivos/train/genders.txt'\n",
    "\n",
    "test_audio_path = 'vivos/test/waves'\n",
    "test_prompts_path = 'vivos/test/prompts.txt'\n",
    "test_genders_path = 'vivos/test/genders.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1aa5dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để đọc file prompts.txt và trả về DataFrame\n",
    "def load_prompts(prompts_path):\n",
    "    transcripts = []\n",
    "    with open(prompts_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            id, text = line.strip().split(' ', 1)\n",
    "            transcripts.append({'id': id, 'text': text.lower()})\n",
    "    return pd.DataFrame(transcripts)\n",
    "\n",
    "# Tạo DataFrame cho tập train và test\n",
    "train_transcripts = load_prompts(train_prompts_path)\n",
    "test_transcripts = load_prompts(test_prompts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67c202e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thêm đường dẫn âm thanh vào DataFrame\n",
    "def get_audio_path(audio_base_path, audio_id):\n",
    "    speaker = audio_id.split('_')[0]\n",
    "    return os.path.join(audio_base_path, speaker, audio_id + '.wav')\n",
    "\n",
    "train_transcripts['audio'] = train_transcripts['id'].apply(lambda x: get_audio_path(train_audio_path, x))\n",
    "test_transcripts['audio'] = test_transcripts['id'].apply(lambda x: get_audio_path(test_audio_path, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5de0f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loại bỏ các ký tự đặc biệt và chuyển văn bản về chữ thường\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"“%‘”�]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch\n",
    "\n",
    "# Áp dụng hàm tiền xử lý dữ liệu\n",
    "train_transcripts = train_transcripts.apply(remove_special_characters, axis=1)\n",
    "test_transcripts = test_transcripts.apply(remove_special_characters, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bac5b126",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/asr-nvlb-24-30/wav2vec2-vivos/checkpoints/checkpoint_epoch_29.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tải processor và model từ mô hình pre-trained\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# processor giữ nguyên do không tham gia vào quá trình fine-tune\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/asr-nvlb-24-30/wav2vec2-vivos/checkpoints/checkpoint_epoch_29.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m processor \u001b[38;5;241m=\u001b[39m Wav2Vec2Processor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/asr-nvlb-24-30/wav2vec2-vivos/checkpoints/processor\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m Wav2Vec2ForCTC\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnguyenvulebinh/wav2vec2-base-vietnamese-250h\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      7\u001b[0m                                        attention_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m,       \u001b[38;5;66;03m# Tăng dropout đế giảm overfitting\u001b[39;00m\n\u001b[0;32m      8\u001b[0m                                        hidden_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, activation_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m,\n\u001b[0;32m      9\u001b[0m                                        ctc_loss_reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m                                        pad_token_id \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n",
      "File \u001b[1;32md:\\app\\python\\lib\\site-packages\\torch\\serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32md:\\app\\python\\lib\\site-packages\\torch\\serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32md:\\app\\python\\lib\\site-packages\\torch\\serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/asr-nvlb-24-30/wav2vec2-vivos/checkpoints/checkpoint_epoch_29.pth'"
     ]
    }
   ],
   "source": [
    "# Tải processor và model từ mô hình pre-trained\n",
    "# processor giữ nguyên do không tham gia vào quá trình fine-tune\n",
    "checkpoint = torch.load(\"/kaggle/input/asr-nvlb-24-30/wav2vec2-vivos/checkpoints/checkpoint_epoch_29.pth\", map_location=device)\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"/kaggle/input/asr-nvlb-24-30/wav2vec2-vivos/checkpoints/processor\") \n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"nguyenvulebinh/wav2vec2-base-vietnamese-250h\", \n",
    "                                       attention_dropout=0.25,       # Tăng dropout đế giảm overfitting\n",
    "                                       hidden_dropout=0.25, activation_dropout=0.25,\n",
    "                                       ctc_loss_reduction=\"mean\",\n",
    "                                       pad_token_id = processor.tokenizer.pad_token_id)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
